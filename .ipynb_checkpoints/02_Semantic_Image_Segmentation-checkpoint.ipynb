{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCSP-dbMw88x"
   },
   "source": [
    "# 1. **Develop solution:**  Semantic Image segmentation for fashion industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMP7mglMuGT2",
    "tags": []
   },
   "source": [
    "This demo focuses on the task of semantic image segmentation for **fashion industry**, using a modified <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\"external\">U-Net</a>.\n",
    "\n",
    "***What is semantic image segmentation?***\n",
    "\n",
    "**Semantic Segmentation** is a classic Computer Vision problem which involves taking as input some raw data (eg., 2D images) and converting them into a mask with regions of interest highlighted. Many use the term full-pixel semantic segmentation, where each pixel in an image is assigned a classID depending on which object of interest it belongs to. Semantic image segmentation is the process of mapping and classifying the natural world for many critical applications such as especially **autonomous driving, robotic navigation, localization, Precision Agriculture, GeoSensing â€“ For land usage, Facial Segmentation, Categorizing clothing items, healthcare and scene understanding**.\n",
    "\n",
    "This workshop uses the Clothing Co-Parsing (CCP) dataset is a new clothing database including elaborately annotated clothing items.(https://github.com/amirgholipour/clothing-co-parsing)\n",
    "\n",
    "The dataset consists of\n",
    "\n",
    "    + 2, 098 high-resolution street fashion photos with totally 59 tags\n",
    "    + Wide range of styles, accessaries, garments, and pose\n",
    "    + All images are with image-level annotations\n",
    "    + 1000+ images are with pixel-level annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll learn how to **do semantic Image segmentation for fashion industry**.\n",
    "\n",
    "This will allow you to discover, step by step, how you can create the code doing the recognition. In the last part of the workshop, this exact same code will be **packaged to create a service** that you can query from any application!\n",
    "\n",
    "We will use a **pre-trained model** as it takes a **long time** and **lots of data** to train such a model. However, you'll find the code to train the model as a reference in this repository if you're interested to learn more.\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.  Install the modeling requirements and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll need to **install some libraries** that are not part of our container image. Normally, **Red Hat OpenShift Data Science** is already taking care of this for you, based on what it detects in the code. **Red Hat OpenShift Data Science** will reinstall all those libraries for you every time you launch the notebook!\n",
    "\n",
    "In case you're using this notebook in a different environment, or just to make sure everything is ready, you can run the following cell to install OpenCV (a library to work with images) and Keras (an abstraction layer over Tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQmKthrSBCld"
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/tensorflow/examples.git\n",
    "%pip install -r requirements.txt\n",
    "%pip install tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.  Importing the needed libraries and packages\n",
    "Of course, we'll need to import various packages. They are either built in the notebook image you are running, or have been installed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQX7R4bhZy5h"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from git import Repo\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "from src.dataloading.read_dataset import readData\n",
    "from src.features.data_preprocessing import preprocessData\n",
    "from src.visualization.visualize import visualizeData, DisplayCallback\n",
    "from src.modules.build_model import buildModel\n",
    "from src.modules.train_model import  trainModel\n",
    "from src.modules.predict_model import predictor\n",
    "from src.hyper_parameters.hps import get_hyper_paras\n",
    "from src.github.git_utils import gitCommands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Initialize some hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH,STEPS_PER_EPOCH,VALIDATION_STEPS,EPOCHS,VAL_SUBSPLITS = get_hyper_paras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWe0_rQM4JbC"
   },
   "source": [
    "## 1.5. Reading the Clothing Co-Parsing (CCP) Dataset\n",
    "\n",
    "The dataset is Clothing Co-Parsing (CCP) Dataset (https://github.com/amirgholipour/clothing-co-parsing.git). The semantic segmentation masks are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images,masks = readData(cwd,link,isdir).readImageData()\n",
    "images,masks = readData().readImageData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.  Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to resize the data to make them ready for feeing to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val,sample_image, sample_mask = preprocessData(images,masks).dataPreProcessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAOe93FRMk3w"
   },
   "source": [
    "## 1.7.  Design and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModel(train_data = train).setupModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8.  Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainModel(model,train_data=train, test_data=val,validation_steps =VALIDATION_STEPS, step_per_epoch = STEPS_PER_EPOCH ,ckpp ='./models/SemImSeg_model_EfficientNetV2B0.h5',  val_subsplits = VAL_SUBSPLITS, batch_size=BATCH,epochs=EPOCHS,sample_image = sample_image ,sample_mask=sample_mask,display_callback =DisplayCallback).modelTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9.  Test Model based on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor(val_data=val, model_path='/models/SemImSeg_model_EfficientNetV2B0.h5').predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10.  Clean dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('rm -rf clone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11. Update the private git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base, repoName = os.path.split(os.getcwd())\n",
    "temp=time.localtime(time.time())\n",
    "uploaddate= 'Update with the latest change ' + str(temp[0])+'_'+str(temp[1])+'_'+str(temp[2])+'_'+str(temp[3])+'_'+str(temp[4])\n",
    "gitCommands(repo_dir = os.getcwd(),repo_name = repoName,git_email= os.environ['GIT_EMAIL'], git_username = os.environ['GIT_USER_NAME'].lower(), git_token = os.environ['GIT_TOKEN'], commit_message = uploaddate, file_name = '.').gitPush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Thank you for your time!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deploy solution for Semantic Image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Move the prediction file to the inference repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "repoName = 'SIS-Inference'\n",
    "repoDir= base +'/'+repoName\n",
    "model_name = '/models/SemImSeg_model_EfficientNetV2B0.h5'\n",
    "shutil.move(os.getcwd()+model_name, base+'/'+repoName+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Push the change to the inference repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gitCommands(repo_dir = repoDir,repo_name = repoName,git_email= email, git_username = username, git_token = token, commit_message = uploaddate, file_name = '.').gitPush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Just need to go to **Redhat Openshift Dedicated** to deploy the app :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segmentation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
